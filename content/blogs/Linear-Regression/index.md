---
title: Linear Regression
subtitle: A review of the linear regression.

# Summary for listings and search engines
summary: This article summarizes the key points of linear regression including assumption, formulization and the model performance.

# Link this post with a project
projects: []

# Date published
date: "2021-11-02T00:00:00Z"

# Date updated
lastmod: "2020-12-13T00:00:00Z"

# Is this an unpublished draft?
draft: false

# Show this page in the Featured widget?
featured: false

# Featured image
# Place an image named `featured.jpg/png` in this page's folder and customize its options here.
image:
  caption: 'Image credit: [**Scikit-learn**](https://scikit-learn.org/stable/auto_examples/linear_model/plot_nnls.html#sphx-glr-auto-examples-linear-model-plot-nnls-py)'
  focal_point: ""
  placement: 2
  preview_only: false

authors:
- admin

tags:
- Machine Learning


categories:
- Review

---
### Introduction

#### What is the assumption of linear regression?

* Assumption: as we change $X$, the only thing that can possibly change about the conditional distribution of $Y$ given $(X_1, . . . , X_k)$, is the conditional mean $E(Yâˆ£X_1, . . . , X_k)$. 

* General form of the regression model: General form of the regression model:
$$
  Y=E(Yâˆ£X_1,â‹¯,X_k)+Z.
$$ 

#### Least-square principle

* Suppose we have a random variable $Y$, and we want to estimate $E(Y)$ based on a sample $(y_1, . . . , y_n)$. The least-squares principle says that we select the point $t(y_1, . . . , y_n)$, in the set of possible values for $E(Y)$, that minimizes the sum of squared  deviations (hence, â€œleast squaresâ€) given by 

$$
  âˆ‘_{i=1}^n(y_iâˆ’t(y_1,â‹¯,y_n))^2. 
$$

Such an estimate is called the **least squares estimate**. 

### Simple Linear Regression

* Formula 

$$
  E((Y_1,\cdots,Y_n)|X_1=x_1, \cdots, X_n=x_n)
$$
$$
  =(\beta_1+\beta_2x_1, \cdots, \beta_1+\beta_2x_n )^T
$$

* LS estimates: 
  * $\beta_1=\bar{y}âˆ’\beta_2\bar{x}$.
  * $\beta_2=\frac{âˆ‘_{i=1}^n(x_iâˆ’\bar{x})(y_iâˆ’\bar{y})}{âˆ‘_{i=1}^n(x_iâˆ’\bar{x})^2}$.
 
* LS predictions: 
  * $E(B_1âˆ£X_1=x_1,â‹¯,X_n=x_n)=\beta_1$.
  * $E(B_2âˆ£X_1=x_1,â‹¯,X_n=x_n)=\beta_2$.
 
* LS standard errors: 
  * $Var(B_1âˆ£X_1=x_1,â‹¯,X_n=x_n)=\sigma^2(\frac{1}{n}+\frac{\bar{x}^2}{âˆ‘_{i=1}^n(x_iâˆ’\bar{x})^2})$.
  * $Var(B_2âˆ£X_1=x_1,â‹¯,X_n=x_n)=\frac{\sigma^2}{âˆ‘_{i=1}^n(x_iâˆ’\bar{x})^2}$.
  * $  E(S_2âˆ£X_1=x_1,â‹¯,X_n=x_n)=\sigma^2$, while: 
$$
  s=\frac{1}{nâˆ’2}âˆ‘_{i=1}^n(y_iâˆ’b_1âˆ’b_2x_1)^2.
$$ 

* ANOVA (analysis of variance table) decomposition: 
$$
  âˆ‘_{i=1}^n(y_iâˆ’\bar{y})^2=b_2^2âˆ‘_{i=1}^n(x_iâˆ’\bar{x})^2
$$
$$
  +âˆ‘_{i=1}^n(y_iâˆ’b_1âˆ’b_2x_i)^2.
$$
  * Regression sum of squares (RSS): measuring changes in the response due to changes in the predictor. 
$$
  RSS=b_2^2âˆ‘_{i=1}^n(x_iâˆ’\bar{x})^2.
$$ 

  * Error sum of squares (ESS): measuring changes in the response due to the contribution of random error. 
$$
  ESS=âˆ‘_{i=1}^n(y_iâˆ’b_1âˆ’b_2x_i)^2.
$$ 

* F-statistics: 
$$
  F=\frac{RSS}{ESS/(nâˆ’2)}
$$ 
$$
  =\frac{b_2^2âˆ‘_{i=1}^n(x_iâˆ’\bar{x})^2}{s^2}âˆ¼F(1,nâˆ’2).
$$

  * If $H_0: \beta_2=0$ is true, then $F=1$, which indicates no relationship between response and predictor. 
  * If $Fâ‰«1$, we should reject $H_0$. 

* ANOVA test or F-test: 
  * Null hypothesis $H_0:\beta_2=0$. 
  * P value: 
$$
  P(Fâ‰¥\frac{b_2^2âˆ‘_{i=1}^n(x_iâˆ’\bar{x})^2}{s^2}).
$$ 

* The Coefficient of Determination: 
  * [Definition] The proportion of the observed variation in the response explained by changes in the predictor: 
$$
  R^2=\frac{b_2^2âˆ‘_{i=1}^n(x_iâˆ’\bar{x})^2}{âˆ‘_{i=1}^n(y_iâˆ’\bar{y})^2}.
$$ 

  * $R^2$ near 1: highly accurate predictions. 
  * $R^2$ near 0: not accurate. 

### General Linear Regression: 

* Matrix form: 
$$
  ğ²=ğ—\bf{\beta}+\bf{\epsilon}.
$$

Where: 

$$
  ğ²=(y_1, y_2, \cdots, y_n)_{n\times1}^T, 
$$

$$
  ğ—=(1, X_{1}, X_{2}, \cdots, X_{d})_{n\times d},
$$

$$
  \beta=(\beta_1, \beta_2, \cdots, \beta_d)_{d\times1}^T, 
$$

$$
  \epsilon=(\epsilon_1, \epsilon_2, \cdots, \epsilon_n)_{n\times1}^T.
$$ 

* The overdetermined system (nâ‰¥d) usually has no exact solution. Then OLS solution is found out by solving the quadratic minimizing problem: 
$$
  \hat{\bf{\beta}}=\underset{{\bf{\beta}âˆˆâ„^d}}{\operatorname{argmin}}\ S(\bf{\beta})
$$

Where: 
$$
  \hat{\bf{\epsilon}}=ğ²âˆ’\hat{\bf{\beta}}ğ—.
$$ 
$$
  S(\bf{\beta})=\hat{\bf{\epsilon}}^T\hat{\bf{\epsilon}}
$$
$$
  =âˆ‘_{i=1}^nâˆ£y_iâˆ’âˆ‘_{j=1}^dx_{ij}\beta_jâˆ£^2=â€–ğ²âˆ’\bf{\beta}ğ—â€–^2
$$
$$
=(ğ²âˆ’\bf{\beta}ğ—)^T(ğ²âˆ’ğ—\bf{\beta}).
$$ 

Provided that the d columns are linearly independent, the minimization has a unique solution: 
$$
  \hat{\bf{\beta}}=(ğ—^Tğ—)^{âˆ’1}ğ—^Tğ².
$$ 

* OLS time complexity: 
$$
  O(nÃ—d^2)
$$ 

* Some terminologies: 
  * Normal equation: 
$$
  (ğ—^Tğ—)\hat{\bf{\beta}}=ğ—^Tğ².
$$ 

  * Normal matrix: 
$$
  ğ—^Tğ—.
$$ 

  * The predicted values: 
$$
  \hat{\bf{y}}=ğ—\hat{\bf{\beta}}=ğ—(ğ—^Tğ—)^{âˆ’1}ğ—^Tğ²=ğğ².
$$ 

  * The projection matrix: 
$$
  ğ=ğ—(ğ—^Tğ—)^{âˆ’1}ğ—^T.
$$ 

  * The annihilator matrix: 
$$
  ğŒ=ğˆ_nâˆ’ğ.
$$ 

  * The "centering" matrix: 
$$
  ğ‚=ğˆ_nâˆ’\frac{1}{n}ğ‰_n,
$$
where $ğ‰_n$ is an n-by-n matrix of all $1$'s. 

* Properties: 
  * Coefficient of determination: 
$$
  R^2=\frac{b_2^2âˆ‘_{i=1}^n(x_iâˆ’\bar{x})^2}{âˆ‘_{i=1}^n(y_iâˆ’\bar{y})^2}
$$ 
$$
=1âˆ’\frac{ğ²^TğŒğ²}{ğ²^Tğ‚ğ²}=1âˆ’\frac{RSS}{TSS}.
$$

  * $ğ—^T\hat{\bf{\epsilon}}=0$.

  * Definitions:
$$
  RSS=âˆ‘_{i=1}^n(y_iâˆ’\hat{y})^2=ğ²^TğŒğ²=\hat{\bf{\epsilon}}^T\hat{\bf{\epsilon}}.
$$ 
$$
  ESS=âˆ‘_{i=1}^n(\hat{y}_iâˆ’\bar{y})^2=ğ²^Tğ^Tğ‹ğğ².
$$ 
$$
  TSS=âˆ‘_{i=1}^n(y_iâˆ’\bar{y})^2=ğ²^Tğ‚ğ².
$$ 
$$
  TSS=RSS+ESS.
$$ 