---
title: Gaussian Mixture Model
subtitle: A review of the Gaussian Mixture Model (GMM) and the Expectationâ€“Maximization (EM) algorithm.

# Summary for listings and search engines
summary: A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters.

# Link this post with a project
projects: []

# Date published
date: "2021-11-01T00:00:00Z"

# Date updated
lastmod: "2020-12-13T00:00:00Z"

# Is this an unpublished draft?
draft: false

# Show this page in the Featured widget?
featured: false

# Featured image
# Place an image named `featured.jpg/png` in this page's folder and customize its options here.
image:
  caption: 'Image credit: [**Scikit-learn**](https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_pdf.html)'
  focal_point: ""
  placement: 2
  preview_only: false

authors:
- admin

tags:
- Machine Learning


categories:
- Review

---

## Overview

1. Gaussian Mixture Model Parameters (k Components): 
  * Clustering probabilities: 
    
    $$ ğœ‹=(ğœ‹_1,â‹¯ğœ‹_k) $$
    
  * Cluster means: 
    * ğœ‡=(ğœ‡1,â‹¯,ğœ‡k)
  * Cluster covariance matrices: 
    * Î£=(Î£1,â‹¯,Î£k)
 
2. Joint Distribution: 

  * p(x,z)=p(z)p(xâˆ£âˆ£z)=ğœ‹zğ’©(xâˆ£âˆ£ğœ‡z,Î£z).
 
  * Notes: 
    * x|z has distribution ğ’©(ğœ‡z,Î£z). z corresponds to (x is the true cluster assignment). 
    * Suppose we know the model parameters ğœ‹z,ğœ‡z,Î£z, then it is easy to evaluate the join density p(x,z). 

Latent Variable Model: 

[Definition] A latent variable model is a probability model for which certain variables are never observed. 

Example: 

In the Gaussian mixture model, we don't observe 
z
 (the cluster assignment). Therefore 
z
 is a latent variable, or a hidden variable. 

The GMM "Inference" Problem: 

Suppose we observe 
x
, we want to know its cluster assignment 
z
. 

The conditional probability for cluster 
z
 given 
x
 is: 

p(zâˆ£âˆ£x)=p(x,y)p(x).
 

The conditional distribution is a soft assignment to clusters. 

A hard assignment is: 

zâˆ—=argmaxzâˆˆ{1,â‹¯,k}p(zâˆ£âˆ£x).
 

If we have the model, the inference is trivial. 

Mixture Models: 

Margin Distribution: 

[Definition] The margin distribution for a single observation 
x
 is: 

p(x)=âˆ‘kz=1p(x,z)=âˆ‘kz=1ğœ‹zğ’©(xâˆ£âˆ£ğœ‡z,Î£z).
 

Notes: 

p(x)
 is a convex combination of probability densities. 

Mixture Distributions (or Mixture Models): 

[Definition] A probability density 
p(x)
 represents a mixture distribution or mixture model, if we can write it as a convex combination of probability densities. That is: 

p(x)=Î£ki=1wipi(x),
 

where 
wiâ‰¥0
, 
Î£ki=1wi=1
, and each 
pi
 is a probability density. 

Notes: 

In the GMM, 
x
 has a mixture distribution. 

More constructively, let 
S
 be a set of probability distributions: 

Choose a distribution randomly from 
S
. 

Sample 
x
 from the chosen distribution. 

Then 
x
 has a mixture distribution. 

The GMM "Learning" Problem: 

Statement of the problem: 

Given data 
x1,â‹¯,xn
 drawn from a GMM. 

Estimate the parameters 
ğœ‹z,ğœ‡z,Î£z
. 

Review: Estimating a Gaussian Distribution: 

The density for 
xâˆ¼ğ’©(ğœ‡,Î£)
 is: 

p(xâˆ£âˆ£ğœ‡,Î£)=1|2ğœ‹Î£|â€¾â€¾â€¾â€¾â€¾âˆšexp(âˆ’12(xâˆ’ğœ‡)TÎ£âˆ’1(xâˆ’ğœ‡)).
 

The log-density is: 

logp(xâˆ£âˆ£ğœ‡,Î£)=âˆ’12log|2ğœ‹Î£|âˆ’12(xâˆ’ğœ‡)TÎ£âˆ’1(xâˆ’ğœ‡).
 

The log joint density from a sample 
x1,â‹¯,xn
 i.i.d. from a 
ğ’©(ğœ‡,Î£)
 distribution is: 

J(ğœ‡,Î£)=âˆ‘ni=1logp(xâˆ£âˆ£ğœ‡,Î£)=âˆ’n2log|2ğœ‹Î£|âˆ’12âˆ‘ni=1(xâˆ’ğœ‡)Tğ›´âˆ’1(xâˆ’ğœ‡).
 

To estimate 
ğœ‡
 and 
Î£
 from a sample 
x1,â‹¯,xn
 i.i.d. from a 
ğ’©(ğœ‡,Î£)
 distribution, we need to maximize the log joint density: 

âˆ‡ğœ‡J(ğœ‡,Î£)=0âŸ¹ğœ‡Ë†MLE=1nâˆ‘ni=1xi.
 

âˆ‡Î£J(ğœ‡,Î£)=0âŸ¹Î£Ë†MLE=1nâˆ‘ni=1(xiâˆ’ğœ‡Ë†MLE)T(xiâˆ’ğœ‡Ë†MLE).
 

Estimating the GMM using maximum likelihood: 

Find parameter values with highest likelihood for the observed data. 

The model likelihood for 
ğ’Ÿ=(x1,â‹¯,xn)
 sampled i.i.d. from a GMM is: 

L(ğœ‹,ğœ‡,Î£)=âˆni=1p(xi)
 

        =âˆni=1âˆkz=1ğœ‹zğ’©(xiâˆ£âˆ£ğœ‡z,Î£z).
 

The objective function is: 

J(ğœ‹,ğœ‡,Î£)=âˆ‘ni=1log{âˆ‘kz=1ğœ‹zğ’©(xiâˆ£âˆ£ğœ‡z,Î£z)}.
 

Plugging in the probability density for 
ğ’©(ğœ‡,Î£)
, we get the GMM log-likelihood: 

J(ğœ‹,ğœ‡,Î£)=âˆ‘ni=1log{âˆ‘kz=1ğœ‹zâˆ£âˆ£2ğœ‹Î£zâˆ£âˆ£â€¾â€¾â€¾â€¾â€¾â€¾âˆšexp(âˆ’12(xâˆ’ğœ‡z)TÎ£âˆ’1(xâˆ’ğœ‡z))}.
 

Issues with MLE for GMM: 

No closed form expression for MLE. 

There are 
k!
 equivalent solutions. 

The likelihood goes to infinity as 
ğœ2âŸ¶0
 for some outliers. 

Keep restarting optimization. 

Bayesian approach. 

Running SGD on the GMM log-likelihood objective can be done in principle, but is challenging. 

The EM Algorithm for GMM: 

Estimating a Fully-Observed GMM: 

Suppose we observe 
(x1,z1),â‹¯,(xn,zn)
 i.i.d. from GMM 
p(x,z)
. Then find MLE is easy: 

nz=âˆ‘ni=1ğŸ(zi=z).
 

ğœ‹Ë†(z)=nzn.
 

ğœ‡Ë†z=1nzâˆ‘i:zi=zxi.
 

Î£Ë†z=1nzâˆ‘i:zi=z(xiâˆ’ğœ‡Ë†z)(xiâˆ’ğœ‡Ë†z)T.
 

Cluster Responsibilities: 

Denote the probability that observed value 
xi
 comes from cluster 
j
 by: 

ğ›¾ji=p(z=jâˆ£âˆ£x=xi),
 

which is the responsibility that cluster 
j
 takes for observation 
xi
. 

Given the parameters 
ğœ‹z,ğœ‡z,Î£z
, it is easy to find: 

ğ›¾ji=p(z=jâˆ£âˆ£xi)
 

      =p(z=j,xi)p(xi)
 

      =ğœ‹jğ’©(xiâˆ£âˆ£ğœ‡j,Î£j)âˆ‘kc=1ğœ‹cğ’©(xiâˆ£âˆ£ğœ‡c,Î£c).
 

The vector 
(ğ›¾1i,â‹¯,ğ›¾ki)
 is exactly the soft assignment for 
xi
. 

Algorithm: 

Algorithm: EM algorithm for GMM 

Input: 
ğ’Ÿ={x1,â‹¯,xn}âŠ‚ğ’³
. 

GMM (
k
 components): 
ğœ‹=(ğœ‹1,â‹¯ğœ‹k)
, 
ğœ‡=(ğœ‡1,â‹¯,ğœ‡k)
, 
Î£=(Î£1,â‹¯,Î£k).
 

Initialize: 
ğœ‹(0),ğœ‡(0),Î£(0),t=0.
 

While not converge: 

For 
i=1,â‹¯,n
 and 
j=1,â‹¯,k
: 

ğ›¾ji=ğœ‹(t)jğ’©(xiâˆ£âˆ£ğœ‡(t)j,Î£(t)j)âˆ‘kc=1ğœ‹cğ’©(xiâˆ£âˆ£ğœ‡c,Î£c)
 // the "E step 

For 
c=1,â‹¯,k
: // the "M step 

nc=âˆ‘ni=1ğ›¾ci.
 

ğœ‡(t+1)câŸµ1ncâˆ‘ni=1ğ›¾cixi.
 

Î£(t+1)câŸµ1ncâˆ‘ni=1ğ›¾ci(xiâˆ’ğœ‡(t+1)c)(xiâˆ’ğœ‡(t+1)c)T.
 

ğœ‹(t+1)câŸµncn.
 

tâŸµt+1
 

Return 
ğœ‹Ë†, ğœ‡Ë†, Î£Ë†
. 

Relation to 
k
-Means: 

If we fix the cluster covariance matrix to be 
ğœ2I
, 

As we take 
ğœ2âŸ¶0
, the update equations converge to doing 
k
-means. 

Soft assignments converg
