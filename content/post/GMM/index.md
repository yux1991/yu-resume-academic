---
title: Gaussian Mixture Model
subtitle: A review of the Gaussian Mixture Model (GMM) and the Expectationâ€“Maximization (EM) algorithm.

# Summary for listings and search engines
summary: A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters.

# Link this post with a project
projects: []

# Date published
date: "2021-11-01T00:00:00Z"

# Date updated
lastmod: "2020-12-13T00:00:00Z"

# Is this an unpublished draft?
draft: false

# Show this page in the Featured widget?
featured: false

# Featured image
# Place an image named `featured.jpg/png` in this page's folder and customize its options here.
image:
  caption: 'Image credit: [**Scikit-learn**](https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_pdf.html)'
  focal_point: ""
  placement: 2
  preview_only: false

authors:
- admin

tags:
- Machine Learning


categories:
- Review

---
### Introduction

#### What is Gaussian Mixture Model?
The Gaussian mixture model has the form:
$$
  f(x)=\sum_{m=1}^M\alpha_m\phi(x;\mu_m,\Sigma_m),
$$
with mixing proportions $\alpha_m$, $\Sigma_m\alpha_m=1$, and each Gaussian density has a mean $\mu_m$ and covariance matrix $\Sigma_m$. One can think of mixture models as generalizing k-means clustering to incorporate information about the covariance structure of the data as well as the centers of the latent Gaussians.

### Overview

* Gaussian Mixture Model Parameters ($k$ Components): 
  * Clustering probabilities: 
    $$ ğœ‹=(ğœ‹_1,â‹¯,ğœ‹_k). $$
  * Cluster means: 
    $$ ğœ‡=(ğœ‡_1,â‹¯,ğœ‡_k). $$
  * Cluster covariance matrices: 
    $$ Î£=(Î£_1,â‹¯,Î£_k). $$
 
* Joint Distribution: 

  $$ p(x,z)=p(z)p(xâˆ£z)=ğœ‹_zğ’©(xâˆ£ğœ‡_z,Î£_z). $$
 
  * Notes: 
    * $x|z$ has distribution $ğ’©(ğœ‡_z,Î£_z)$. $z$ corresponds to ($x$ is the true cluster assignment). 
    * Suppose we know the model parameters $ğœ‹_z$, $ğœ‡_z$, $Î£_z$, then it is easy to evaluate the join density $p(x,z)$. 

* Latent Variable Model: 
  * A latent variable model is a probability model for which certain variables are never observed. 
  * In the Gaussian mixture model, we don't observe $z$ (the cluster assignment). Therefore $z$ is a latent variable, or a hidden variable. 

### The GMM "Inference" Problem

* Suppose we observe $x$, we want to know its cluster assignment $z$. 
* The conditional probability for cluster $z$ given $x$ is: 
$$
  p(zâˆ£x)=p(x,y)p(x).
$$

* The conditional distribution is a soft assignment to clusters. 

* A hard assignment is: 
$$
  z^âˆ—=argmax_{zâˆˆ\{1,â‹¯,k\}}p(zâˆ£x).
$$

* If we have the model, the inference is trivial. 

### Mixture Models: 

* Margin Distribution: 
  * The margin distribution for a single observation $x$ is: 
$$
  p(x)=âˆ‘_{z=1}^k p(x,z)=âˆ‘_{z=1}^k ğœ‹_zğ’©(xâˆ£ğœ‡_z,Î£_z).
$$

  * Notes: 
    * $p(x)$ is a convex combination of probability densities. 

* Mixture Distributions (or Mixture Models): 
  * A probability density $p(x)$ represents a mixture distribution or mixture model, if we can write it as a convex combination of probability densities. That is: 
$$
  p(x)=Î£_{i=1}^k w_ip_i(x),
$$ 

where $w_iâ‰¥0$, $Î£_{i=1}^k w_i=1$, and each $pi$ is a probability density. 

  * Notes: 
    * In the GMM, $x$ has a mixture distribution. 
    * More constructively, let $S$ be a set of probability distributions: 
      * Choose a distribution randomly from $S$. 
      * Sample $x$ from the chosen distribution. 
      * Then $x$ has a mixture distribution. 

### The GMM "Learning" Problem: 

* Statement of the problem: Given data $x_1,â‹¯,x_n$ drawn from a GMM. Estimate the parameters $ğœ‹_z$, $ğœ‡_z$, $Î£_z$. 
* Review: Estimating a Gaussian Distribution: 
  * The density for $xâˆ¼ğ’©(ğœ‡,Î£)$ is: 
$$
  p(xâˆ£ğœ‡,Î£)=\frac{1}{\sqrt{|2ğœ‹Î£|}}\exp{âˆ’1/2(xâˆ’ğœ‡)^TÎ£^{âˆ’1}(xâˆ’ğœ‡)}.
$$

  * The log-density is: 
$$
  log p(xâˆ£ğœ‡,Î£)=âˆ’1/2log|2ğœ‹Î£|âˆ’1/2(xâˆ’ğœ‡)^TÎ£^{âˆ’1}(xâˆ’ğœ‡).
$$

  * The log joint density from a sample $x_1,â‹¯,x_n$ i.i.d. from a $ğ’©(ğœ‡,Î£)$ distribution is: 

  $$
    J(ğœ‡,Î£)=âˆ‘_{i=1}^n log p(xâˆ£ğœ‡,Î£)
  $$
  $$
    =âˆ’n/2log|2ğœ‹Î£|âˆ’1/2âˆ‘_{i=1}^n(xâˆ’ğœ‡)^Tğ›´^{âˆ’1}(xâˆ’ğœ‡).
  $$

  * To estimate $ğœ‡$ and $Î£$ from a sample $x_1,â‹¯,x_n$ i.i.d. from a $ğ’©(ğœ‡,Î£)$ distribution, we need to maximize the log joint density: 

  $$
    âˆ‡_ğœ‡J(ğœ‡,Î£)=0âŸ¹\hat{ğœ‡}_{MLE}
  $$
  
  $$
    =1/nâˆ‘_{i=1}^n x_i.
  $$
  
  $$
    âˆ‡_Î£J(ğœ‡,Î£)=0âŸ¹\hat{Î£}_{MLE}
  $$
  
  $$
    =1/nâˆ‘_{i=1}^n (x_iâˆ’\hat{ğœ‡}_{MLE})^T(x_iâˆ’\hat{ğœ‡}_{MLE}).
  $$

* Estimating the GMM using maximum likelihood: 

  * Find parameter values with highest likelihood for the observed data. 
  * The model likelihood for $ğ’Ÿ=(x_1,â‹¯,x_n)$ sampled i.i.d. from a GMM is: 

  $$
    L(ğœ‹,ğœ‡,Î£)=âˆ_{i=1}^n p(x_i)
  $$
  $$
    =âˆ_{i=1}^n âˆ_{z=1}^k ğœ‹_zğ’©(x_iâˆ£ğœ‡_z,Î£_z).
  $$

  * The objective function is: 

  $$
    J(ğœ‹,ğœ‡,Î£)=âˆ‘_{i=1}^nlog\{âˆ‘_{z=1}^kğœ‹_zğ’©(x_iâˆ£ğœ‡_z,Î£_z)\}.
  $$ 

  * Plugging in the probability density for $ğ’©(ğœ‡,Î£)$, we get the GMM log-likelihood:

  $$
    J(ğœ‹,ğœ‡,Î£)=
  $$
  $$
    âˆ‘_{i=1}^n logâˆ‘_{z=1}^k \frac{ğœ‹_z}{\sqrt{âˆ£2ğœ‹Î£_zâˆ£}} \exp{âˆ’1/2(xâˆ’ğœ‡_z)^TÎ£^{âˆ’1}(xâˆ’ğœ‡_z)}.
  $$
  

  * Issues with MLE for GMM: 
    * No closed form expression for MLE. 
    * There are $k!$ equivalent solutions. 
    * The likelihood goes to infinity as $ğœ^2âŸ¶0$ for some outliers. 
      * Keep restarting optimization. 
      * Bayesian approach. 
    * Running SGD on the GMM log-likelihood objective can be done in principle, but is challenging. 

### The EM Algorithm for GMM: 

* Estimating a Fully-Observed GMM: 
  * Suppose we observe $(x_1,z_1),â‹¯(x_n,z_n)$ i.i.d. from GMM $p(x,z)$. Then find MLE is easy: 

  $$
    n_z=âˆ‘_i^n=1(z_i=z),
    \hat{ğœ‹}(z)=n_z/n.
  $$

  $$
    \hat{\mu}_z=1/n_z
  $$
  
  $$\times\Sigma_{i:z_i=z}x_i.$$

  $$
    \hat{\Sigma}_z=1/n_z
  $$
  
  $$\times\Sigma_{i:z_i=z}(x_iâˆ’\hat{\mu}_z)(x_iâˆ’\hat{\mu}_z)^T.$$

* Cluster Responsibilities: 
  * Denote the probability that observed value $xi$ comes from cluster $j$ by: 

  $$
    ğ›¾_i^j=p(z=jâˆ£x=x_i),
  $$

which is the responsibility that cluster $j$ takes for observation $x_i$. 

  * Given the parameters $ğœ‹_z,ğœ‡_z,Î£_z$, it is easy to find: 

  $$
    ğ›¾_i^j=p(z=jâˆ£x_i)
  $$
  $$
    =p(z=j,x_i)p(x_i)
  $$
  $$
    =ğœ‹_jğ’©(x_iâˆ£ğœ‡_j,Î£_j)âˆ‘_{c=1}^k ğœ‹_cğ’©(x_iâˆ£ğœ‡_c,Î£_c).
  $$

  * The vector $(ğ›¾_i^1,â‹¯,ğ›¾_i^k)$ is exactly the soft assignment for $x_i$. 

* Algorithm
> * **Algorithm: EM algorithm for GMM** 
>  ---
>    * **Input**: 
>      * $ğ’Ÿ={x_1,â‹¯,x_n}âŠ‚ğ’³$. 
>      * GMM ($k$ components): 
>   
>   $$
>     ğœ‹=(ğœ‹_1,â‹¯ğœ‹_k),
>   $$
>   $$
>     ğœ‡=(ğœ‡_1,â‹¯,ğœ‡_k),
>   $$
>   $$
>     Î£=(Î£_1,â‹¯,Î£_k).
>   $$ 
>
>    * **Initialize**: 
>  $$
>    ğœ‹(0),ğœ‡(0),Î£(0),t=0.
>  $$ 
>
>    * **While not converge**: 
>      * For $i=1,â‹¯,n$ and $j=1,â‹¯,k$: 
>   $$
>     ğ›¾_i^j=\frac{ğœ‹_j^{(t)}ğ’©(x_iâˆ£ğœ‡_j^{(t)},Î£_j^{(t)})}{âˆ‘_{c=1}^kğœ‹_cğ’©(x_iâˆ£ğœ‡_c,Î£_c)}
>   $$ // the "E step 
>
>      * For $c=1,â‹¯,k$: // the "M step 
>   
>   $$
>     n_c=âˆ‘_{i=1}^nğ›¾_i^c.
>   $$
>   $$
>     ğœ‡_c^{(t+1)}âŸµ1/n_câˆ‘_{i=1}^nğ›¾_i^c x_i.
>   $$
>   $$
>     Î£_c^{(t+1)}âŸµ1/n_câˆ‘_{i=1}^nğ›¾_i^c (x_iâˆ’ğœ‡_c^{(t+1)})(x_iâˆ’ğœ‡_c^{(t+1)})^T.
>   $$
>   $$
>     ğœ‹_c^{(t+1)}âŸµn_c/n.
>   $$
>   $$
>     tâŸµt+1
>   $$
>
>    * **Return** $\hat{ğœ‹}, \hat{ğœ‡}, \hat{Î£}$. 

* Relation to k-Means: 
  * If we fix the cluster covariance matrix to be $ğœ^2I$, 
  * As we take $ğœ^2âŸ¶0$, the update equations converge to doing k-means. 
  * Soft assignments converge to hard assignments (because of the tail behavior of Gaussian).
